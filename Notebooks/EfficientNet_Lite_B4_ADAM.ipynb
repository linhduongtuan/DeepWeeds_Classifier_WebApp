{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33965,
     "status": "ok",
     "timestamp": 1569356388112,
     "user": {
      "displayName": "APMHE15-VN APMHE15-VN",
      "photoUrl": "",
      "userId": "18025187755721220904"
     },
     "user_tz": -420
    },
    "id": "ahVeBxHeaypW",
    "outputId": "110ee9b1-efee-4ec8-f612-ad8edf7d7053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 19 11:21:29 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   35C    P5    22W / 250W |   1227MiB / 11176MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1245      G   /usr/lib/xorg/Xorg                           164MiB |\n",
      "|    0      1488      G   /usr/bin/gnome-shell                         121MiB |\n",
      "|    0      2770      G   ...AAAAAAAAAAAAAAgAAAAAAAAA --shared-files    81MiB |\n",
      "|    0     26844      C   /home/linh/.conda/envs/CV/bin/python         855MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPwL9bdoBNzQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import geffnet\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from geffnet import create_model\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 113041,
     "status": "ok",
     "timestamp": 1569356467203,
     "user": {
      "displayName": "APMHE15-VN APMHE15-VN",
      "photoUrl": "",
      "userId": "18025187755721220904"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "7614ef22-2f41-4175-f265-79b129a33153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 14007, 'val': 3502}\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/DeepWeeds/'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/val'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# Using the image datasets and the trainforms, define the data_loader\n",
    "# batch_size = 64 for EfficientNet from B0 - B3\n",
    "# batch_size = 32 for EfficientNet B4, B5\n",
    "# batch_size = 16 for EfficientNet_B6\n",
    "# batch_size = 8 for EfficientNet_B7\n",
    "# batch_size = 32 for MixNet_s\n",
    "batch_size = 16\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "\"\"\"# Label mapping\n",
    "with open('/home/linh/Downloads/Derma/cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)\"\"\"\n",
    "\n",
    "''' \n",
    "f = open('/home/linh/Downloads/Derma/classes.txt','r')\n",
    "cat_to_name = f.read()\n",
    "print(cat_to_name)\n",
    "f.close()\n",
    "'''\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['val']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 121421,
     "status": "ok",
     "timestamp": 1569356475598,
     "user": {
      "displayName": "APMHE15-VN APMHE15-VN",
      "photoUrl": "",
      "userId": "18025187755721220904"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "0eb1b0c1-37d9-4538-fab8-87072bcbb2c7"
   },
   "outputs": [],
   "source": [
    "#model = EfficientNet.from_pretrained('efficientnet-b3')\n",
    "#model = timm.create_model('tf_efficientnet_b6', pretrained = True)\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "#model = EfficientNet.from_pretrained('efficientnet-b6')\n",
    "model = create_model('tf_efficientnet_lite4', pretrained=True)\n",
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "n_classes = 9\n",
    "model.classifier = nn.Linear(model.classifier.in_features, n_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chinee apple',\n",
       " 'Lantana',\n",
       " 'Negative',\n",
       " 'Parkinsonia',\n",
       " 'Parthenium',\n",
       " 'Prickly acacia',\n",
       " 'Rubber vine',\n",
       " 'Siam weed',\n",
       " 'Snake weed']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "cat_to_name\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record loss is SAVED: {epoch_loss}')\n",
    " \n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1569385104347,
     "user": {
      "displayName": "APMHE15-VN APMHE15-VN",
      "photoUrl": "",
      "userId": "18025187755721220904"
     },
     "user_tz": -420
    },
    "id": "vcXkJFOlP4NJ",
    "outputId": "2d1cd376-c190-4d2e-ac32-c61057918c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.09322643780425104, Val accuracy: 0.9691695906432749\n",
      "Epoch 0/99\n",
      "----------\n",
      "[1, 999] loss: 0.21498878\n",
      "[1, 1999] loss: 0.21604584\n",
      "[1, 2999] loss: 0.21728260\n",
      "[1, 3999] loss: 0.21537541\n",
      "train Loss: 0.21522474 Acc: 0.92673684\n",
      "[1, 999] loss: 0.09245275\n",
      "val Loss: 0.09273952 Acc: 0.96987135\n",
      "New best model found!\n",
      "New record loss: 0.09273952202559911, previous record loss: 0.09322643780425104\n",
      "New record loss is SAVED: 0.09273952202559911\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "[2, 999] loss: 0.21608521\n",
      "[2, 1999] loss: 0.20994701\n",
      "[2, 2999] loss: 0.21205027\n",
      "[2, 3999] loss: 0.21395211\n",
      "train Loss: 0.21392724 Acc: 0.92647173\n",
      "[2, 999] loss: 0.09632084\n",
      "val Loss: 0.09476769 Acc: 0.96893567\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "[3, 999] loss: 0.21729812\n",
      "[3, 1999] loss: 0.21565302\n",
      "[3, 2999] loss: 0.21457699\n",
      "[3, 3999] loss: 0.21547103\n",
      "train Loss: 0.21545175 Acc: 0.92764133\n",
      "[3, 999] loss: 0.09482938\n",
      "val Loss: 0.09519588 Acc: 0.96898246\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "[4, 999] loss: 0.21647196\n",
      "[4, 1999] loss: 0.21168381\n",
      "[4, 2999] loss: 0.21335212\n",
      "[4, 3999] loss: 0.21362507\n",
      "train Loss: 0.21362614 Acc: 0.92800000\n",
      "[4, 999] loss: 0.09211766\n",
      "val Loss: 0.09443562 Acc: 0.96879532\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "[5, 999] loss: 0.21387715\n",
      "[5, 1999] loss: 0.21542804\n",
      "[5, 2999] loss: 0.21742543\n",
      "[5, 3999] loss: 0.21761875\n",
      "train Loss: 0.21768377 Acc: 0.92519298\n",
      "[5, 999] loss: 0.09265781\n",
      "val Loss: 0.09557395 Acc: 0.96776608\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "[6, 999] loss: 0.20994450\n",
      "[6, 1999] loss: 0.21268337\n",
      "[6, 2999] loss: 0.21472397\n",
      "[6, 3999] loss: 0.21344492\n",
      "train Loss: 0.21343997 Acc: 0.92807797\n",
      "[6, 999] loss: 0.10141599\n",
      "val Loss: 0.09862052 Acc: 0.96711111\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "[7, 999] loss: 0.21944766\n",
      "[7, 1999] loss: 0.22307851\n",
      "[7, 2999] loss: 0.21970285\n",
      "[7, 3999] loss: 0.22001106\n",
      "train Loss: 0.21998700 Acc: 0.92391423\n",
      "[7, 999] loss: 0.09676774\n",
      "val Loss: 0.09490093 Acc: 0.96954386\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "[8, 999] loss: 0.21437967\n",
      "[8, 1999] loss: 0.21333941\n",
      "[8, 2999] loss: 0.21460521\n",
      "[8, 3999] loss: 0.21550468\n",
      "train Loss: 0.21556352 Acc: 0.92673684\n",
      "[8, 999] loss: 0.09596987\n",
      "val Loss: 0.09845608 Acc: 0.96762573\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "[9, 999] loss: 0.20639666\n",
      "[9, 1999] loss: 0.21193856\n",
      "[9, 2999] loss: 0.21595871\n",
      "[9, 3999] loss: 0.21828068\n",
      "train Loss: 0.21832608 Acc: 0.92615984\n",
      "[9, 999] loss: 0.09817340\n",
      "val Loss: 0.09579157 Acc: 0.96842105\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "[10, 999] loss: 0.21185604\n",
      "[10, 1999] loss: 0.21415356\n",
      "[10, 2999] loss: 0.21872633\n",
      "[10, 3999] loss: 0.21924824\n",
      "train Loss: 0.21966204 Acc: 0.92603509\n",
      "[10, 999] loss: 0.09194672\n",
      "val Loss: 0.09103734 Acc: 0.97024561\n",
      "New best model found!\n",
      "New record loss: 0.09103734142633907, previous record loss: 0.09273952202559911\n",
      "New record loss is SAVED: 0.09103734142633907\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "[11, 999] loss: 0.22026655\n",
      "[11, 1999] loss: 0.21609494\n",
      "[11, 2999] loss: 0.21609397\n",
      "[11, 3999] loss: 0.21558874\n",
      "train Loss: 0.21560120 Acc: 0.92658090\n",
      "[11, 999] loss: 0.09404218\n",
      "val Loss: 0.09397451 Acc: 0.96851462\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "[12, 999] loss: 0.21450048\n",
      "[12, 1999] loss: 0.21513633\n",
      "[12, 2999] loss: 0.21645347\n",
      "[12, 3999] loss: 0.21511172\n",
      "train Loss: 0.21506025 Acc: 0.92595712\n",
      "[12, 999] loss: 0.09266633\n",
      "val Loss: 0.09423715 Acc: 0.96870175\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "[13, 999] loss: 0.21217773\n",
      "[13, 1999] loss: 0.21250551\n",
      "[13, 2999] loss: 0.21253080\n",
      "[13, 3999] loss: 0.21486599\n",
      "train Loss: 0.21477456 Acc: 0.92589474\n",
      "[13, 999] loss: 0.09997606\n",
      "val Loss: 0.09797305 Acc: 0.96814035\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "[14, 999] loss: 0.21742882\n",
      "[14, 1999] loss: 0.21616067\n",
      "[14, 2999] loss: 0.21681370\n",
      "[14, 3999] loss: 0.21515973\n",
      "train Loss: 0.21514843 Acc: 0.92687719\n",
      "[14, 999] loss: 0.09430241\n",
      "val Loss: 0.09590089 Acc: 0.96832749\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "[15, 999] loss: 0.21715866\n",
      "[15, 1999] loss: 0.21558931\n",
      "[15, 2999] loss: 0.21384850\n",
      "[15, 3999] loss: 0.21581225\n",
      "train Loss: 0.21601882 Acc: 0.92659649\n",
      "[15, 999] loss: 0.09596386\n",
      "val Loss: 0.09649704 Acc: 0.96771930\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "[16, 999] loss: 0.21026052\n",
      "[16, 1999] loss: 0.21355113\n",
      "[16, 2999] loss: 0.21567148\n",
      "[16, 3999] loss: 0.21597504\n",
      "train Loss: 0.21601361 Acc: 0.92658090\n",
      "[16, 999] loss: 0.09539152\n",
      "val Loss: 0.09493063 Acc: 0.96935673\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "[17, 999] loss: 0.22022465\n",
      "[17, 1999] loss: 0.21770605\n",
      "[17, 2999] loss: 0.21494707\n",
      "[17, 3999] loss: 0.21450841\n",
      "train Loss: 0.21448239 Acc: 0.92658090\n",
      "[17, 999] loss: 0.10181091\n",
      "val Loss: 0.09955487 Acc: 0.96697076\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "[18, 999] loss: 0.21456037\n",
      "[18, 1999] loss: 0.21940493\n",
      "[18, 2999] loss: 0.21798417\n",
      "[18, 3999] loss: 0.21868446\n",
      "train Loss: 0.21854643 Acc: 0.92640936\n",
      "[18, 999] loss: 0.09193157\n",
      "val Loss: 0.09282326 Acc: 0.96940351\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "[19, 999] loss: 0.21029898\n",
      "[19, 1999] loss: 0.21246543\n",
      "[19, 2999] loss: 0.21158003\n",
      "[19, 3999] loss: 0.21274340\n",
      "train Loss: 0.21271041 Acc: 0.92800000\n",
      "[19, 999] loss: 0.09415168\n",
      "val Loss: 0.09452112 Acc: 0.96888889\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "[20, 999] loss: 0.21759047\n",
      "[20, 1999] loss: 0.21768762\n",
      "[20, 2999] loss: 0.21491616\n",
      "[20, 3999] loss: 0.21490336\n",
      "train Loss: 0.21492949 Acc: 0.92630019\n",
      "[20, 999] loss: 0.09550632\n",
      "val Loss: 0.09622148 Acc: 0.96874854\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "[21, 999] loss: 0.21662244\n",
      "[21, 1999] loss: 0.21774929\n",
      "[21, 2999] loss: 0.21897266\n",
      "[21, 3999] loss: 0.21790624\n",
      "train Loss: 0.21795313 Acc: 0.92558285\n",
      "[21, 999] loss: 0.09812985\n",
      "val Loss: 0.09701870 Acc: 0.96846784\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "[22, 999] loss: 0.22160972\n",
      "[22, 1999] loss: 0.21776863\n",
      "[22, 2999] loss: 0.21823133\n",
      "[22, 3999] loss: 0.21662948\n",
      "train Loss: 0.21661156 Acc: 0.92732943\n",
      "[22, 999] loss: 0.10082469\n",
      "val Loss: 0.10064727 Acc: 0.96743860\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "[23, 999] loss: 0.21684961\n",
      "[23, 1999] loss: 0.22110342\n",
      "[23, 2999] loss: 0.22276717\n",
      "[23, 3999] loss: 0.22037558\n",
      "train Loss: 0.22053088 Acc: 0.92586355\n",
      "[23, 999] loss: 0.09488278\n",
      "val Loss: 0.09419103 Acc: 0.96870175\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "[24, 999] loss: 0.21522034\n",
      "[24, 1999] loss: 0.21563696\n",
      "[24, 2999] loss: 0.21580050\n",
      "[24, 3999] loss: 0.21605752\n",
      "train Loss: 0.21596087 Acc: 0.92586355\n",
      "[24, 999] loss: 0.09363579\n",
      "val Loss: 0.09680208 Acc: 0.96814035\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "[25, 999] loss: 0.22122173\n",
      "[25, 1999] loss: 0.22187624\n",
      "[25, 2999] loss: 0.21947797\n",
      "[25, 3999] loss: 0.21984081\n",
      "train Loss: 0.21982997 Acc: 0.92467836\n",
      "[25, 999] loss: 0.09998027\n",
      "val Loss: 0.09782167 Acc: 0.96753216\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "[26, 999] loss: 0.21156722\n",
      "[26, 1999] loss: 0.21414537\n",
      "[26, 2999] loss: 0.21608985\n",
      "[26, 3999] loss: 0.22003759\n",
      "train Loss: 0.21990057 Acc: 0.92550487\n",
      "[26, 999] loss: 0.09778245\n",
      "val Loss: 0.09738049 Acc: 0.96879532\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "[27, 999] loss: 0.22247726\n",
      "[27, 1999] loss: 0.21601231\n",
      "[27, 2999] loss: 0.21814781\n",
      "[27, 3999] loss: 0.21652232\n",
      "train Loss: 0.21652399 Acc: 0.92561404\n",
      "[27, 999] loss: 0.09958912\n",
      "val Loss: 0.09804688 Acc: 0.96739181\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "[28, 999] loss: 0.21872318\n",
      "[28, 1999] loss: 0.22172832\n",
      "[28, 2999] loss: 0.22223599\n",
      "[28, 3999] loss: 0.22338639\n",
      "train Loss: 0.22350552 Acc: 0.92383626\n",
      "[28, 999] loss: 0.09078464\n",
      "val Loss: 0.09287388 Acc: 0.96926316\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "[29, 999] loss: 0.21905058\n",
      "[29, 1999] loss: 0.21979187\n",
      "[29, 2999] loss: 0.21897218\n",
      "[29, 3999] loss: 0.21931392\n",
      "train Loss: 0.21935989 Acc: 0.92739181\n",
      "[29, 999] loss: 0.09590751\n",
      "val Loss: 0.09473222 Acc: 0.96907602\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "[30, 999] loss: 0.21882420\n",
      "[30, 1999] loss: 0.21601208\n",
      "[30, 2999] loss: 0.21469915\n",
      "[30, 3999] loss: 0.21500645\n",
      "train Loss: 0.21518587 Acc: 0.92601949\n",
      "[30, 999] loss: 0.09414935\n",
      "val Loss: 0.09319950 Acc: 0.96870175\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "[31, 999] loss: 0.20590268\n",
      "[31, 1999] loss: 0.21006189\n",
      "[31, 2999] loss: 0.21202335\n",
      "[31, 3999] loss: 0.21365588\n",
      "train Loss: 0.21373698 Acc: 0.92650292\n",
      "[31, 999] loss: 0.09533102\n",
      "val Loss: 0.09462963 Acc: 0.96837427\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "[32, 999] loss: 0.22411102\n",
      "[32, 1999] loss: 0.21741497\n",
      "[32, 2999] loss: 0.21935121\n",
      "[32, 3999] loss: 0.21747450\n",
      "train Loss: 0.21782123 Acc: 0.92489669\n",
      "[32, 999] loss: 0.09334136\n",
      "val Loss: 0.09383483 Acc: 0.96842105\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "[33, 999] loss: 0.22482938\n",
      "[33, 1999] loss: 0.22047948\n",
      "[33, 2999] loss: 0.21690009\n",
      "[33, 3999] loss: 0.21708883\n",
      "train Loss: 0.21750756 Acc: 0.92633138\n",
      "[33, 999] loss: 0.09728022\n",
      "val Loss: 0.09733243 Acc: 0.96701754\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "[34, 999] loss: 0.21711695\n",
      "[34, 1999] loss: 0.21835871\n",
      "[34, 2999] loss: 0.21557359\n",
      "[34, 3999] loss: 0.21552802\n",
      "train Loss: 0.21563686 Acc: 0.92601949\n",
      "[34, 999] loss: 0.09946499\n",
      "val Loss: 0.09873128 Acc: 0.96748538\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "[35, 999] loss: 0.21916944\n",
      "[35, 1999] loss: 0.22288928\n",
      "[35, 2999] loss: 0.22180500\n",
      "[35, 3999] loss: 0.21848463\n",
      "train Loss: 0.21838018 Acc: 0.92614425\n",
      "[35, 999] loss: 0.09662695\n",
      "val Loss: 0.09444177 Acc: 0.96935673\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "[36, 999] loss: 0.22182599\n",
      "[36, 1999] loss: 0.21920011\n",
      "[36, 2999] loss: 0.21855286\n",
      "[36, 3999] loss: 0.21756452\n",
      "train Loss: 0.21741559 Acc: 0.92642495\n",
      "[36, 999] loss: 0.09952985\n",
      "val Loss: 0.09645536 Acc: 0.96790643\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "[37, 999] loss: 0.22139968\n",
      "[37, 1999] loss: 0.21925049\n",
      "[37, 2999] loss: 0.21756844\n",
      "[37, 3999] loss: 0.21653280\n",
      "train Loss: 0.21628030 Acc: 0.92584795\n",
      "[37, 999] loss: 0.09058504\n",
      "val Loss: 0.09505801 Acc: 0.96940351\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "[38, 999] loss: 0.21964506\n",
      "[38, 1999] loss: 0.21347727\n",
      "[38, 2999] loss: 0.21388825\n",
      "[38, 3999] loss: 0.21336307\n",
      "train Loss: 0.21332992 Acc: 0.92762573\n",
      "[38, 999] loss: 0.09931253\n",
      "val Loss: 0.09722455 Acc: 0.96818713\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "[39, 999] loss: 0.22145670\n",
      "[39, 1999] loss: 0.22427247\n",
      "[39, 2999] loss: 0.21914912\n",
      "[39, 3999] loss: 0.21872484\n",
      "train Loss: 0.21870922 Acc: 0.92508382\n",
      "[39, 999] loss: 0.09509192\n",
      "val Loss: 0.09440872 Acc: 0.96856140\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "[40, 999] loss: 0.21036596\n",
      "[40, 1999] loss: 0.21073125\n",
      "[40, 2999] loss: 0.21321374\n",
      "[40, 3999] loss: 0.21501040\n",
      "train Loss: 0.21499547 Acc: 0.92778168\n",
      "[40, 999] loss: 0.09473127\n",
      "val Loss: 0.09692322 Acc: 0.96842105\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "[41, 999] loss: 0.21638433\n",
      "[41, 1999] loss: 0.21319086\n",
      "[41, 2999] loss: 0.21394619\n",
      "[41, 3999] loss: 0.21378909\n",
      "train Loss: 0.21376999 Acc: 0.92654971\n",
      "[41, 999] loss: 0.09830720\n",
      "val Loss: 0.09648691 Acc: 0.96818713\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "[42, 999] loss: 0.21893376\n",
      "[42, 1999] loss: 0.21657203\n",
      "[42, 2999] loss: 0.21628461\n",
      "[42, 3999] loss: 0.21762828\n",
      "train Loss: 0.21761732 Acc: 0.92654971\n",
      "[42, 999] loss: 0.09358970\n",
      "val Loss: 0.09430291 Acc: 0.96973099\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/DeepWeeds/weights/EfficientNet_Lite4_ADAM.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 100,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EfficientNet_B6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
