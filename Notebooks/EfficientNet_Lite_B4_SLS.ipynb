{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPwL9bdoBNzQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sls\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import geffnet\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 113041,
     "status": "ok",
     "timestamp": 1569356467203,
     "user": {
      "displayName": "APMHE15-VN APMHE15-VN",
      "photoUrl": "",
      "userId": "18025187755721220904"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "7614ef22-2f41-4175-f265-79b129a33153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 64125, 'val': 21375}\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/DeepWeeds/'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/val'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #transforms.RandomRotation(30),\n",
    "        #transforms.RandomResizedCrop(224),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# Using the image datasets and the trainforms, define the data_loader\n",
    "# batch_size = 64 for EfficientNet from B0 - B3\n",
    "# batch_size = 32 for EfficientNet B4, B5\n",
    "# batch_size = 16 for EfficientNet_B6\n",
    "# batch_size = 8 for EfficientNet_B7\n",
    "# batch_size = 32 for MixNet_s\n",
    "batch_size = 16\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "\"\"\"# Label mapping\n",
    "with open('/home/linh/Downloads/Derma/cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)\"\"\"\n",
    "\n",
    "\n",
    "'''f = open('/home/linh/Downloads/Derma/classes.txt','r')\n",
    "cat_to_name = f.read()\n",
    "print(cat_to_name)\n",
    "f.close()\n",
    "'''\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['val']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 121421,
     "status": "ok",
     "timestamp": 1569356475598,
     "user": {
      "displayName": "APMHE15-VN APMHE15-VN",
      "photoUrl": "",
      "userId": "18025187755721220904"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "0eb1b0c1-37d9-4538-fab8-87072bcbb2c7"
   },
   "outputs": [],
   "source": [
    "#model = EfficientNet.from_pretrained('efficientnet-b3')\n",
    "model = create_model('tf_efficientnet_lite4', pretrained = True)\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "#model = EfficientNet.from_pretrained('efficientnet-b6')\n",
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "n_classes = 9\n",
    "model.classifier = nn.Linear(model.classifier.in_features, n_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = sls.Sls(model.parameters())\n",
    "#optimizer = sls.Sls(model.parameters(), n_batches_per_epoch=len(data_loader['train']))\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "def train_model(model, criterion, optimizer, \n",
    "                #scheduler, \n",
    "                num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        #scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # establish function for SLS optimization\n",
    "                def closure():\n",
    "                    probs = F.log_softmax(model(inputs), dim=1)\n",
    "                    loss = F.nll_loss(probs, labels, reduction=\"sum\")\n",
    "                    \n",
    "                    return loss\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        #loss.backward()\n",
    "                        optimizer.step(closure)\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            #'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record loss is SAVED: {epoch_loss}')\n",
    "      \n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1569385104347,
     "user": {
      "displayName": "APMHE15-VN APMHE15-VN",
      "photoUrl": "",
      "userId": "18025187755721220904"
     },
     "user_tz": -420
    },
    "id": "vcXkJFOlP4NJ",
    "outputId": "2d1cd376-c190-4d2e-ac32-c61057918c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint not found\n",
      "Epoch 0/99\n",
      "----------\n",
      "[1, 999] loss: 0.85259798\n",
      "[1, 1999] loss: 0.64890996\n",
      "[1, 2999] loss: 0.54884999\n",
      "[1, 3999] loss: 0.48149250\n",
      "train Loss: 0.48111378 Acc: 0.83566472\n",
      "[1, 999] loss: 0.18625599\n",
      "val Loss: 0.18986047 Acc: 0.93861988\n",
      "New best model found!\n",
      "New record loss: 0.18986046899689568, previous record loss: inf\n",
      "New record loss is SAVED: 0.18986046899689568\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "[2, 999] loss: 0.15698305\n",
      "[2, 1999] loss: 0.15254926\n",
      "[2, 2999] loss: 0.14496978\n",
      "[2, 3999] loss: 0.13907779\n",
      "train Loss: 0.13901545 Acc: 0.95345029\n",
      "[2, 999] loss: 0.14480696\n",
      "val Loss: 0.14847752 Acc: 0.95134503\n",
      "New best model found!\n",
      "New record loss: 0.14847752456236304, previous record loss: 0.18986046899689568\n",
      "New record loss is SAVED: 0.14847752456236304\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "[3, 999] loss: 0.07657205\n",
      "[3, 1999] loss: 0.07350872\n",
      "[3, 2999] loss: 0.07020978\n",
      "[3, 3999] loss: 0.06847991\n",
      "train Loss: 0.06842704 Acc: 0.97808967\n",
      "[3, 999] loss: 0.13714367\n",
      "val Loss: 0.13702746 Acc: 0.95719298\n",
      "New best model found!\n",
      "New record loss: 0.13702746419183778, previous record loss: 0.14847752456236304\n",
      "New record loss is SAVED: 0.13702746419183778\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "[4, 999] loss: 0.04160776\n",
      "[4, 1999] loss: 0.04086264\n",
      "[4, 2999] loss: 0.04002045\n",
      "[4, 3999] loss: 0.03939648\n",
      "train Loss: 0.03936421 Acc: 0.98792982\n",
      "[4, 999] loss: 0.12583324\n",
      "val Loss: 0.13290328 Acc: 0.96149708\n",
      "New best model found!\n",
      "New record loss: 0.1329032774249974, previous record loss: 0.13702746419183778\n",
      "New record loss is SAVED: 0.1329032774249974\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "[5, 999] loss: 0.02652413\n",
      "[5, 1999] loss: 0.02764643\n",
      "[5, 2999] loss: 0.02802730\n",
      "[5, 3999] loss: 0.02733526\n",
      "train Loss: 0.02729538 Acc: 0.99192203\n",
      "[5, 999] loss: 0.14462417\n",
      "val Loss: 0.14491648 Acc: 0.96079532\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "[6, 999] loss: 0.01968762\n",
      "[6, 1999] loss: 0.02118151\n",
      "[6, 2999] loss: 0.02143586\n",
      "[6, 3999] loss: 0.02106719\n",
      "train Loss: 0.02108708 Acc: 0.99390253\n",
      "[6, 999] loss: 0.13021435\n",
      "val Loss: 0.13320982 Acc: 0.96336842\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "[7, 999] loss: 0.01493497\n",
      "[7, 1999] loss: 0.01738096\n",
      "[7, 2999] loss: 0.01783431\n",
      "[7, 3999] loss: 0.01714198\n",
      "train Loss: 0.01715378 Acc: 0.99482261\n",
      "[7, 999] loss: 0.12663876\n",
      "val Loss: 0.13073032 Acc: 0.96542690\n",
      "New best model found!\n",
      "New record loss: 0.13073032258407413, previous record loss: 0.1329032774249974\n",
      "New record loss is SAVED: 0.13073032258407413\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "[8, 999] loss: 0.01417861\n",
      "[8, 1999] loss: 0.01359479\n",
      "[8, 2999] loss: 0.01412897\n",
      "[8, 3999] loss: 0.01416403\n",
      "train Loss: 0.01416095 Acc: 0.99631969\n",
      "[8, 999] loss: 0.14365688\n",
      "val Loss: 0.14809401 Acc: 0.96187135\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "[9, 999] loss: 0.01483845\n",
      "[9, 1999] loss: 0.01535043\n",
      "[9, 2999] loss: 0.01483379\n",
      "[9, 3999] loss: 0.01471408\n",
      "train Loss: 0.01471978 Acc: 0.99613255\n",
      "[9, 999] loss: 0.13216348\n",
      "val Loss: 0.13032515 Acc: 0.96566082\n",
      "New best model found!\n",
      "New record loss: 0.13032515444393047, previous record loss: 0.13073032258407413\n",
      "New record loss is SAVED: 0.13032515444393047\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "[10, 999] loss: 0.01495657\n",
      "[10, 1999] loss: 0.01546134\n",
      "[10, 2999] loss: 0.01515492\n",
      "[10, 3999] loss: 0.01517014\n",
      "train Loss: 0.01519204 Acc: 0.99585185\n",
      "[10, 999] loss: 0.14174229\n",
      "val Loss: 0.14048075 Acc: 0.96116959\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "[11, 999] loss: 0.01415713\n",
      "[11, 1999] loss: 0.01370541\n",
      "[11, 2999] loss: 0.01377680\n",
      "[11, 3999] loss: 0.01374120\n",
      "train Loss: 0.01374348 Acc: 0.99617934\n",
      "[11, 999] loss: 0.12837855\n",
      "val Loss: 0.13033454 Acc: 0.96598830\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "[12, 999] loss: 0.01466598\n",
      "[12, 1999] loss: 0.01484295\n",
      "[12, 2999] loss: 0.01405660\n",
      "[12, 3999] loss: 0.01440786\n",
      "train Loss: 0.01441889 Acc: 0.99610136\n",
      "[12, 999] loss: 0.12513571\n",
      "val Loss: 0.12985296 Acc: 0.96552047\n",
      "New best model found!\n",
      "New record loss: 0.12985296167912538, previous record loss: 0.13032515444393047\n",
      "New record loss is SAVED: 0.12985296167912538\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "[13, 999] loss: 0.01410754\n",
      "[13, 1999] loss: 0.01347250\n",
      "[13, 2999] loss: 0.01398201\n",
      "[13, 3999] loss: 0.01415146\n",
      "train Loss: 0.01424162 Acc: 0.99622612\n",
      "[13, 999] loss: 0.13826060\n",
      "val Loss: 0.13819698 Acc: 0.96327485\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "[14, 999] loss: 0.01534120\n",
      "[14, 1999] loss: 0.01565444\n",
      "[14, 2999] loss: 0.01501685\n",
      "[14, 3999] loss: 0.01452972\n",
      "train Loss: 0.01456352 Acc: 0.99597661\n",
      "[14, 999] loss: 0.12544424\n",
      "val Loss: 0.12825711 Acc: 0.96552047\n",
      "New best model found!\n",
      "New record loss: 0.12825711319181654, previous record loss: 0.12985296167912538\n",
      "New record loss is SAVED: 0.12825711319181654\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "[15, 999] loss: 0.01381347\n",
      "[15, 1999] loss: 0.01489737\n",
      "[15, 2999] loss: 0.01504529\n",
      "[15, 3999] loss: 0.01474878\n",
      "train Loss: 0.01473563 Acc: 0.99591423\n",
      "[15, 999] loss: 0.13196805\n",
      "val Loss: 0.12937530 Acc: 0.96575439\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "[16, 999] loss: 0.01371981\n",
      "[16, 1999] loss: 0.01407318\n",
      "[16, 2999] loss: 0.01388642\n",
      "[16, 3999] loss: 0.01448259\n",
      "train Loss: 0.01455297 Acc: 0.99602339\n",
      "[16, 999] loss: 0.13664482\n",
      "val Loss: 0.13980184 Acc: 0.96187135\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "[17, 999] loss: 0.01344229\n",
      "[17, 1999] loss: 0.01538371\n",
      "[17, 2999] loss: 0.01501674\n",
      "[17, 3999] loss: 0.01551937\n",
      "train Loss: 0.01550543 Acc: 0.99553996\n",
      "[17, 999] loss: 0.14424895\n",
      "val Loss: 0.15074830 Acc: 0.96037427\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "[18, 999] loss: 0.01395418\n",
      "[18, 1999] loss: 0.01360509\n",
      "[18, 2999] loss: 0.01457365\n",
      "[18, 3999] loss: 0.01428836\n",
      "train Loss: 0.01427592 Acc: 0.99610136\n",
      "[18, 999] loss: 0.13998125\n",
      "val Loss: 0.13315842 Acc: 0.96491228\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "[19, 999] loss: 0.01523082\n",
      "[19, 1999] loss: 0.01486920\n",
      "[19, 2999] loss: 0.01459797\n",
      "[19, 3999] loss: 0.01501350\n",
      "train Loss: 0.01498934 Acc: 0.99586745\n",
      "[19, 999] loss: 0.14740731\n",
      "val Loss: 0.14883738 Acc: 0.96098246\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "[20, 999] loss: 0.01526674\n",
      "[20, 1999] loss: 0.01434572\n",
      "[20, 2999] loss: 0.01448500\n",
      "[20, 3999] loss: 0.01451812\n",
      "train Loss: 0.01454787 Acc: 0.99617934\n",
      "[20, 999] loss: 0.13059331\n",
      "val Loss: 0.13039166 Acc: 0.96453801\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "[21, 999] loss: 0.01435809\n",
      "[21, 1999] loss: 0.01417769\n",
      "[21, 2999] loss: 0.01442097\n",
      "[21, 3999] loss: 0.01431068\n",
      "train Loss: 0.01430531 Acc: 0.99622612\n",
      "[21, 999] loss: 0.13182650\n",
      "val Loss: 0.13185934 Acc: 0.96523977\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "[22, 999] loss: 0.01524491\n",
      "[22, 1999] loss: 0.01495327\n",
      "[22, 2999] loss: 0.01466857\n",
      "[22, 3999] loss: 0.01451081\n",
      "train Loss: 0.01451558 Acc: 0.99591423\n",
      "[22, 999] loss: 0.13088732\n",
      "val Loss: 0.13027260 Acc: 0.96622222\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "[23, 999] loss: 0.01713095\n",
      "[23, 1999] loss: 0.01549665\n",
      "[23, 2999] loss: 0.01503122\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/DeepWeeds/weights/EfficientNet_Lite4_AP_SLS.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 #scheduler,\n",
    "                                                 num_epochs = 100,\n",
    "                                                 checkpoint = None #torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            #'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EfficientNet_B6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
